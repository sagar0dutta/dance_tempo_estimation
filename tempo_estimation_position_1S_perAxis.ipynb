{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.generators import Triangle\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter, argrelmin, argrelmax\n",
    "from foot_module import onset_calculations, onset_extraction, onset_filtering, utils, onset_plot\n",
    "\n",
    "from load_djembe_marker import *\n",
    "from compute_tempo_pos import *\n",
    "from dance_evaluation import *\n",
    "from mutils import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mvnfiles = [\"BKO_E1_D1_01_Suku_T.mvnx\", \"BKO_E1_D1_02_Maraka_T.mvnx\", \"BKO_E1_D1_03_Wasulunka_T.mvnx\", \"BKO_E1_D2_04_Maraka_T.mvnx\"]\n",
    "\n",
    "# filename = mvnfiles[2]\n",
    "# piece_name = os.path.basename(filename).split(\".\")[0]\n",
    "# data_handler = DataHandler()\n",
    "# motion_data, drum_onsets, start_f, end_f, start_t, end_t, cycle_onsets, beat_ref, bpm = data_handler.load_and_process_data(filename, mode = 'gr', drum = 'J2', section_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BKO_E1_D1_08_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_01_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_04_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_01_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_13_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_05_Sandia_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E1_D1_02_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_14_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_01_Maraka_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_06_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_02_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_12_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_07_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E1_D5_02_Dansa_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E2_D3_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_05_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_03_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_13_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E2_D3_11_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D6_04_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_07_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_04_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_10_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_12_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Results saved to ./results/pos_1s/au/tempo_70_145/foot/left_foot_zero_uni_70_145.csv\n",
      "Loaded BKO_E1_D1_08_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_01_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_04_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_01_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_13_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_05_Sandia_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E1_D1_02_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_14_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_01_Maraka_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_06_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_02_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D4_12_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_07_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E1_D5_02_Dansa_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E2_D3_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_05_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_04_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_03_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_13_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E2_D3_11_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D6_04_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_01_Maraka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D1_05_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D2_07_Sandia_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E2_D3_03_Wasulunka_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E1_D5_04_Suku_T.pkl\n",
      "Error encountered for: Group 'au' not found in the dataset.\n",
      "Loaded BKO_E3_D5_06_Manjanin_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_10_Dansa_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D5_02_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Loaded BKO_E3_D6_12_Suku_T.pkl\n",
      "Total Sections: 1\n",
      "Results saved to ./results/pos_1s/au/tempo_70_145/foot/left_foot_zero_bi_70_145.csv\n"
     ]
    }
   ],
   "source": [
    "# SEGMENT_HEAD  SEGMENT_PELVIS   SEGMENT_T8  SEGMENT_LEFT_HAND  SEGMENT_LEFT_FOOT\n",
    "pkl_filelist = os.listdir(f\"/itf-fi-ml/home/sagardu/djembe_drive/sgr_pyspace/Dataset_V2\")\n",
    "data_handler = DataHandler()\n",
    "\n",
    "bpm_recording = []\n",
    "modes = ['zero_uni', \"zero_bi\"]\n",
    "for mode in modes:\n",
    "    result = {\n",
    "    \"piece\": [],\n",
    "    \"bpm\": [],\n",
    "    \"X_a\": [],\n",
    "    \"Y_a\": [],\n",
    "    \"Z_a\": [],\n",
    "    \"X_b\": [],\n",
    "    \"Y_b\": [],\n",
    "    \"Z_b\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "    \"mode_x\": [],\n",
    "    \"mode_y\": [],\n",
    "    \"mode_z\": [],\n",
    "    \"median_x\": [],\n",
    "    \"median_y\": [],\n",
    "    \"median_z\": [],\n",
    "    }\n",
    "    for idx, filename in enumerate(pkl_filelist):\n",
    "          \n",
    "        filename = filename.replace(\"_Dancers.csv\", \"_T.mvnx\")\n",
    "        piece_name = os.path.basename(filename).split(\".\")[0]\n",
    "        \n",
    "        if piece_name == \"BKO_E2_D3_04_Dansa_T\":\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            motion_data, drum_onsets, start_f, end_f, start_t, end_t, cycle_onsets, beat_ref, bpm = data_handler.load_and_process_data(filename, mode = 'au', drum = 'J2', section_idx=0)\n",
    "        except ValueError as e:\n",
    "            # Handle the error and continue\n",
    "            print(f\"Error encountered for: {e}\")\n",
    "            continue\n",
    "\n",
    "        duration = int(end_t-start_t)\n",
    "        w_sec = int(duration)\n",
    "        h_sec = int(w_sec/4)\n",
    "\n",
    "        mocap_fps = 240\n",
    "        window_size = int(240*w_sec)\n",
    "        hop_size = int(240*h_sec)\n",
    "        \n",
    "        a = 70; b = 145\n",
    "        tempi_range = np.arange(a,b,1)   # good: 70,145   \n",
    "        subdir = \"foot\"\n",
    "        limb = \"left_foot\"\n",
    "        \n",
    "        sensorA_pos_data = motion_data['position'][\"SEGMENT_LEFT_FOOT\"][start_f:end_f, :]\n",
    "        sensorA_position = detrend_signal_array(sensorA_pos_data, cutoff= 0.5)\n",
    "\n",
    "        \n",
    "        novelty_length = len(sensorA_pos_data)\n",
    "        time_axis = np.arange(novelty_length)/mocap_fps\n",
    "        bpm_axes = []\n",
    "        mag_axes = []\n",
    "        for ax in range(3):\n",
    "            pos_min, pos_max = np.min(sensorA_position[:, ax]), np.max(sensorA_position[:, ax])\n",
    "            sensorA_position_norm = (\n",
    "                2*(sensorA_position[:, ax] - pos_min) / (pos_max - pos_min) - 1\n",
    "                if pos_max != pos_min \n",
    "                else np.zeros_like(sensorA_position[:, ax])\n",
    "            )\n",
    "            \n",
    "            sensorA_pos = sensorA_position_norm.reshape(-1,1)\n",
    "\n",
    "            tempo_json_one_sensor = main_one_sensor_peraxis(sensorA_pos,\n",
    "                                                            mocap_fps, window_size, hop_size, tempi_range, \n",
    "                                                            T_filter=0.25, mode=mode)\n",
    "\n",
    "            # sensor_abs_vel = tempo_json_one_sensor[\"sensor_abs_vel\"]\n",
    "            # sensor_dir_onsets = tempo_json_one_sensor[\"sensor_dir_change_onsets\"]\n",
    "            # sensor_dir_onsets_f = tempo_json_one_sensor[\"sensor_dir_change_onsets_f\"]\n",
    "            sensor_onsets = tempo_json_one_sensor[\"sensor_onsets\"]\n",
    "\n",
    "            # tempogram_ab = tempo_json_one_sensor[\"tempogram_ab\"]\n",
    "            # time_axis_seconds = tempo_json_one_sensor[\"time_axis_seconds\"]\n",
    "            # tempo_axis_bpm = tempo_json_one_sensor[\"tempo_axis_bpm\"]\n",
    "\n",
    "            tempo_data_maxmethod = tempo_json_one_sensor[\"tempo_data_maxmethod\"]\n",
    "            tempo_data_weightedkernel = tempo_json_one_sensor[\"tempo_data_weightedkernel\"]\n",
    "            tempo_data_topN = tempo_json_one_sensor[\"tempo_data_topN\"]\n",
    "\n",
    "            # Max method\n",
    "            # Aestimated_beat_pulse = tempo_data_maxmethod[\"estimated_beat_pulse\"]\n",
    "            # Atempo_curve = tempo_data_maxmethod[\"tempo_curve\"]\n",
    "            \n",
    "            bpmA_arr = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "            magA_arr = tempo_data_maxmethod[\"mag_arr\"]\n",
    "            bpmB_arr = tempo_data_weightedkernel[\"bpm_arr\"]\n",
    "            \n",
    "            tempo_A = np.round(np.average(bpmA_arr), 2)\n",
    "            tempo_B = np.round(np.average(bpmB_arr), 2)\n",
    "\n",
    "\n",
    "            # Weighted method\n",
    "            # Bestimated_beat_pulse = tempo_data_weightedkernel[\"estimated_beat_pulse\"]\n",
    "            # Btempo_curve = tempo_data_weightedkernel[\"tempo_curve\"]\n",
    "            # bpmB_arr = tempo_data_weightedkernel[\"bpm_arr\"]\n",
    "            # Btempo_curve_time_axis = tempo_data_weightedkernel[\"tempo_curve_time_axis\"]\n",
    "            # Bglobal_tempo_bpm = tempo_data_weightedkernel[\"global_tempo_bpm\"]\n",
    "\n",
    "            # combined_axis_onsets = np.sum(sensor_dir_onsets, axis=1)     # combine the onsets from 3 axes\n",
    "            # combined_axis_onsets = np.where(combined_axis_onsets > 0, 1,0)\n",
    "            # combined_axis_onsets = filter_dir_onsets_by_threshold(combined_axis_onsets.reshape(-1,1), threshold_s= 0.25)\n",
    "            # combined_axis_onsets = combined_axis_onsets.flatten()   # Binary onsets\n",
    "\n",
    "            # dance_onset, estimated_beat_onset, drum_ref, dance_onset_iois, estimated_beats_iois = data_handler.onsets_for_plotting(sensor_dir_onsets_f, Aestimated_beat_pulse, novelty_length)\n",
    "            # dance_bpm = data_handler.calc_tempo_from_onsets(dance_onset)\n",
    "\n",
    "\n",
    "\n",
    "            bpm_axes.append(bpmA_arr)\n",
    "            \n",
    "            mode_x = stats.mode(bpmA_arr.flatten())[0]\n",
    "            mode_y = stats.mode(bpmA_arr.flatten())[0]\n",
    "            mode_z = stats.mode(bpmA_arr.flatten())[0]\n",
    "\n",
    "            median_x = np.median(bpmA_arr.flatten())\n",
    "            median_y = np.median(bpmA_arr.flatten())\n",
    "            median_z = np.median(bpmA_arr.flatten())\n",
    "            \n",
    "            if ax == 0:\n",
    "                result[\"piece\"].append(piece_name)\n",
    "                result[\"bpm\"].append(np.round(bpm,2))\n",
    "                \n",
    "                result[\"mode_x\"].append(mode_x)\n",
    "                result[\"mode_y\"].append(mode_y)\n",
    "                result[\"mode_z\"].append(mode_z)\n",
    "                \n",
    "                result[\"median_x\"].append(median_x)\n",
    "                result[\"median_y\"].append(median_y)\n",
    "                result[\"median_z\"].append(median_z)\n",
    "                \n",
    "                result[\"X_a\"].append(tempo_A)\n",
    "                result[\"X_b\"].append(tempo_B)\n",
    "            elif ax == 1:\n",
    "                result[\"Y_a\"].append(tempo_A)\n",
    "                result[\"Y_b\"].append(tempo_B)\n",
    "            elif ax == 2:\n",
    "                result[\"Z_a\"].append(tempo_A)\n",
    "                result[\"Z_b\"].append(tempo_B)\n",
    "\n",
    "            # print(f\"bpmA_{ax}:\", bpmA_arr)\n",
    "            # print(\"Average A:\", tempo_A)                           # [start_f:end_f]\n",
    "\n",
    "            \n",
    "            mag_axes.append(magA_arr)\n",
    "            mag_axes_arr = np.column_stack(mag_axes)    # n by 3 array\n",
    "        \n",
    "        bpm_axes_arr = np.column_stack(bpm_axes)    # n by 3 array\n",
    "        bpm_mode = stats.mode(bpm_axes_arr.flatten())[0]\n",
    "        bpm_median = np.median(bpm_axes_arr.flatten())\n",
    "        result[\"bpm_mode\"].append(bpm_mode)\n",
    "        result[\"bpm_median\"].append(bpm_median)\n",
    "        \n",
    "        bpm_recording.append((bpm, bpm_axes_arr, mag_axes_arr))\n",
    "\n",
    "            \n",
    "    results_df = pd.DataFrame(result)\n",
    "    csv_filename = f\"./results/pos_1s/au/tempo_{a}_{b}/{subdir}/{limb}_{mode}_{a}_{b}.csv\"\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results saved to {csv_filename}\")  \n",
    "\n",
    "# 94 115 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpm_axes_arr is 4by3 array with rows: bpm value for frames and columns: x,y z \n",
    "\n",
    "mode_list = []\n",
    "median_list = []\n",
    "for i in range(len(bpm_recording)):\n",
    "    # i = 10\n",
    "    bpm = bpm_recording[i][0]\n",
    "    bpm_axes_arr = bpm_recording[i][1]\n",
    "    mag_axes_arr = bpm_recording[i][2]\n",
    "    bpm_flat = bpm_axes_arr.flatten()\n",
    "    sort_idx = np.argsort(bpm_axes_arr.flatten())\n",
    "    min_n = bpm_flat[sort_idx[:3]]\n",
    "\n",
    "    # error1 = [np.log2(bpm_axes_arr[j,:] / min_n[0]) for j in range(bpm_axes_arr.shape[0])]\n",
    "    # error1_ar = np.vstack(np.round(error1,2))\n",
    "\n",
    "    modes = np.array([\n",
    "    stats.mode(bpm_axes_arr[:, 0], axis=None)[0],\n",
    "    stats.mode(bpm_axes_arr[:, 1], axis=None)[0],\n",
    "    stats.mode(bpm_axes_arr[:, 2], axis=None)[0],\n",
    "    stats.mode(bpm_axes_arr, axis=None)[0]\n",
    "    ])\n",
    "\n",
    "    medians = np.array([\n",
    "        np.median(bpm_axes_arr[:, 0]),\n",
    "        np.median(bpm_axes_arr[:, 1]),\n",
    "        np.median(bpm_axes_arr[:, 2]),\n",
    "        np.median(bpm_axes_arr)\n",
    "    ])\n",
    "    \n",
    "    mode_list.append(modes)\n",
    "    median_list.append(medians)\n",
    "    print(\"\\nRef BPM:\",round(bpm))\n",
    "    # print(\"Mode BPM X:\",stats.mode(bpm_axes_arr[:,0].flatten())[0])\n",
    "    # print(\"Mode BPM Y:\",stats.mode(bpm_axes_arr[:,1].flatten())[0])\n",
    "    # print(\"Mode BPM Z:\",stats.mode(bpm_axes_arr[:,2].flatten())[0])\n",
    "    print(\"Mode BPM XYZ:\",stats.mode(bpm_axes_arr.flatten())[0])\n",
    "\n",
    "    # print(\"\\nMedian BPM X:\", np.median(bpm_axes_arr[:,0].flatten()))\n",
    "    # print(\"Median BPM Y:\", np.median(bpm_axes_arr[:,1].flatten()))\n",
    "    # print(\"Median BPM Z:\", np.median(bpm_axes_arr[:,2].flatten()))\n",
    "    print(\"Median BPM XYZ:\", np.median(bpm_axes_arr.flatten()))\n",
    "    \n",
    "    # print(\"\\nMode BPM F1:\",stats.mode(bpm_axes_arr[0,:].flatten())[0])\n",
    "    # print(\"Mode BPM F2:\",stats.mode(bpm_axes_arr[1,:].flatten())[0])\n",
    "    # print(\"Mode BPM F3:\",stats.mode(bpm_axes_arr[2,:].flatten())[0])\n",
    "    # print(\"Mode BPM F4:\",stats.mode(bpm_axes_arr[3,:].flatten())[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpm_recording[14][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bpm)\n",
    "print(bpm_axes_arr)\n",
    "print(error1_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 6), dpi=200)\n",
    "plt.plot(sensor_onsets)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(40, 6), dpi=200)\n",
    "plt.plot(Atempo_curve[:len(sensor_onsets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempogram_ab, tempogram_raw, time_axis_seconds, tempo_axis_bpm = compute_tempogram(sensor_abs_vel[start_f:end_f], mocap_fps, \n",
    "                                                                     window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(6,6), dpi=100)\n",
    "\n",
    "cax1 = axs.pcolormesh(time_axis_seconds, tempo_axis_bpm, tempogram_ab[0], shading='auto', cmap='magma')\n",
    "axs.set_title('X-axis')\n",
    "axs.set_xlabel('Time [s]')\n",
    "axs.set_ylabel('Tempo [BPM]')\n",
    "plt.colorbar(cax1, ax=axs, orientation='horizontal', label='Magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tempogram_perAxis(tempo_json_one_sensor, islog= 'no', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 6), dpi=200)\n",
    "plt.plot(100*Aestimated_beat_pulse[start_f:end_f], linewidth=1, color = 'b')\n",
    "plt.plot(Atempo_curve[start_f:end_f], linewidth=1, color = 'r')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Tempo (BPM)')\n",
    "plt.title(f'{piece_name} Start:{round(start_t)} End:{round(end_t)}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_b = np.where(sensorA_position[start_f:end_f]> 0, 1,0)\n",
    "plt.figure(figsize=(40, 6), dpi=300)\n",
    "\n",
    "plt.plot(sensor_abs_vel[start_f:end_f], linewidth = 1.0, color='b')\n",
    "plt.plot(pos_b, linewidth = 1.0, color='b')\n",
    "# plt.plot(sensor_dir_onsets_f[start_f:end_f], linewidth = 1.0, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per mode: drum onset and directional change onset plot\n",
    "\n",
    "plt.figure(figsize=(40, 6), dpi=300)\n",
    "window_size = 0.1\n",
    "for onset in beat_ref:\n",
    "    window_start = onset - (window_size/2)  # Start of the window (25ms before)\n",
    "    window_end = onset + (window_size/2)   # End of the window (25ms after)\n",
    "    \n",
    "    # Plot shaded window\n",
    "    plt.axvspan(window_start, window_end, color='red', alpha=0.3)\n",
    "    # Plot reference onset as a vertical line\n",
    "    plt.axvline(onset, color='red', linestyle='--', linewidth=0.9)\n",
    "\n",
    "plt.vlines(x= dance_onset, ymin=0.0, ymax=1, color='g', linewidth=1.2,)\n",
    "# plt.vlines(x= estimated_beat_onset, ymin=0.0, ymax=1, color='b', linewidth=1.5,)\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.title(f'{piece_name}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_dance_onsets_with_half_beats(beat_ref, dance_onset, tolerance=0.2)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(novelty_length) / mocap_fps\n",
    "peaks, properties = signal.find_peaks(Aestimated_beat_pulse)  # , prominence=0.02\n",
    "beat_peaks_sec = time[peaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_duration = 50  # milliseconds\n",
    "click_freq = 1200  # Hz\n",
    "file_name =\"maraka\"\n",
    "# Generate a single click sound\n",
    "click = Triangle(click_freq).to_audio_segment(duration=click_duration)\n",
    "\n",
    "onset_times = beat_peaks_sec  # kept_onsets/240   beat_peaks_sec\n",
    "dN = novelty_length\n",
    "total_duration = (dN/240)*1000  #  in milliseconds\n",
    "\n",
    "audio = AudioSegment.silent(duration=total_duration)\n",
    "for onset in onset_times:\n",
    "    position = int(onset * 1000)  # Convert onset time to milliseconds\n",
    "    audio = audio.overlay(click, position=position)\n",
    "\n",
    "# Export the audio with clicks to a file\n",
    "audio.export(os.path.join(\"/itf-fi-ml/home/sagardu/extract_feet_onset\", f\"{file_name}_Both_Foot_new.wav\"), format=\"wav\")\n",
    "# audio.export(os.path.join(\"/itf-fi-ml/home/sagardu/extract_feet_onset\", f\"{file_name}_Bothhand_dir.wav\"), format=\"wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
